
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
from torch.nn.utils.rnn import pad_sequence
import os
import math
import random
import csv
import datetime
# from sklearn.metrics import mean_squared_error, r2_scor

# Define a custom dataset class
class Traj():
    def __init__(self, data, sw ,sl, shift):
        self.data = data
        self.ln = len(self.data[0])
        self.slidlen = self.nslides(sw ,sl, shift) if self.nslides(sw ,sl, shift) > 0 else 0
        self.zone = torch.zeros(1,5)
        self.zone[0,self.data[-1]] = 1
        self.input_size = input_size
    
    def nslides(self, sw ,sl, shift):
        return (self.ln - sl - shift) // sw + 1

    def slid(self,i, sw ,sl, shift): # Sliding window = 2, sequence length = 10 and shift = 1
        inpt = [row[i*sw: i*sw + sl] for row in self.data[0:self.input_size]]
        #trgt = [row[i*sw +sl: i*sw + 2*sl] for row in self.data[:2]]
        #trgt = [row[i*sw + shift: i*sw + sl + shift] for row in self.data[0:self.output_size]]
        #self.inpt.append(input)
        #self.targ.append(target)
        return inpt, self.zone


class TrajDataset(Dataset):
    def __init__(self, data, targets):
        self.data = data
        self.targets = targets
    def __len__(self):
        return len(self.targets)
    def __getitem__(self, idx):
        return self.data[idx], self.targets[idx]


# # Define the LSTM model
class CoordinateRegressionNN(nn.Module):
    def __init__(self, sequence_length, input_size, hidden_size, num_layers, output_size):
        super(CoordinateRegressionNN, self).__init__()
        
        # Embedding layer to convert coordinates to numerical values
        #self.embedding = nn.Embedding(hidden_size, sequence_length)
        self.embedding = nn.Linear(input_size * sequence_length, sequence_length)
        # LSTM layer to handle sequential information
        self.lstm = nn.LSTM(sequence_length, hidden_size, num_layers, batch_first=True)
        
        # Fully connected layers for regression
        self.fc1 = nn.Linear(hidden_size, 5)
        #self.fc2 = nn.Linear(64, output_size)
        # Activation function
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)
        
    def forward(self, x):
        # Convert coordinates to embeddings
        x =x.view(x.size(0), sequence_length *input_size)
        embedded = self.embedding(x)
        # LSTM layer
        out, _ = self.lstm(embedded)
        # Extract the output of the last time step
        # Fully connected layers
        out = self.relu(self.fc1(out))
        out = self.softmax(out)
        return out


# These function are for loading the saved model or saving them
def save_checkpoint(state, filename='my_checkpoint.pth.tar'):
  print('=> Saving checkpoint')
  torch.save(state, filename)

def load_checkpoint(checkpoint):
  print('=> Loading checkpoint')
  model.load_state_dict(checkpoint['state_dict'])
  optimizer.load_state_dict(checkpoint['optimizer'])

def test_model(model, test_loader):
            print("Starting testing phase")
            model.eval()
            future = 0
            with torch.no_grad():
                Predicteddata = torch.empty(0, zones_num, device=device)
                test_losses = []
                test_loss_num = 0
                for x, y in test_loader:
                    test_loss_num += x.size(0)
                    outputs = model(x)
                    # y = (y - re_mean) / torch.sqrt(re_var + 1e-5)
                    tests_loss = criterion(outputs, y)
                    test_losses.append(tests_loss.item())
                    # outputs = outputs*torch.sqrt(re_var + 1e-5) + re_mean
                    # y = y*torch.sqrt(re_var + 1e-5) + re_mean
                    Predicteddata = torch.cat((Predicteddata, outputs), dim=0)
                    #loss_test.append(test_loss.item())
                avg_test_loss = sum(test_losses) / test_loss_num
                # Calculate accuracy as the inverse of MSE
                accuracy = 1.0 / (1.0 + avg_test_loss)
                print(f"Average Test Loss: {avg_test_loss:.6f}")
                print(f"Accuracy: {accuracy:.6f}")
                return Predicteddata, avg_test_loss
            
def savecsvresult(pred , groundx, groundy):
    cwd = os.getcwd()
    ct = datetime.datetime.now().strftime(r"%m%dT%H%M")
    csvpath = os.path.join(cwd,'Processed',f'Predicteddata{ct}.csv')
    with open(csvpath, 'w',newline='') as file:
        writer = csv.writer(file)
        for i in range(len(groundx)):
            #get the index of the max predicted value
            rowx = torch.argmax(pred[i,:])
            grx = torch.argmax(groundx[i,:])
            # grxx = groundy[i,:,0].tolist()
            # gryy = groundy[i,:,1].tolist()
            writer.writerow([rowx,grx])



if __name__ == '__main__':
    # Load the data
    cwd = os.getcwd()
    csvpath = os.path.join(cwd,'Processed','Zones20231222T1926.csv')
    print(csvpath)
    # # Hyperparameters
    
    input_size = 2
    hidden_size = 1024
    num_layers = 1
    output_size = 1
    epochs = 100
    learning_rate = 1e-3
    batch_size = 256  # Since each batch is a single set
    sequence_length = 50
    sw = 5
    zones_num = 5
    sl = sequence_length
    shift = 0
    df = []
    q= 0
    add_class = True #  Add class by loading the CSV file
    load_class = not add_class      # load the class from the saved file
    prep_data = True # It's for preparing data
    Train = True # It's for training the model with the prepared data
    test_in_epoch = True # It's for testing the model in each epoch
    model_from_scratch = True # It's for creating model from scratch
    load_model = not model_from_scratch # It's for loading model

    with open(csvpath, 'r',newline='') as file:
        for line in file:
            row = line.strip().split(',')
            rowf = [int(float(element)) for element in row]
            rowf = [0 if math.isnan(x) else x for x in rowf]
            df.append(rowf)

    #load brokentraj.csv file
    # csvpath = os.path.join(cwd,'brokentraj.csv')
    # with open(csvpath, 'r') as file:
    #     for line in file:
    #         row = line.strip().split(',')
    #         blcklist = list(map(int,row))
    # print('blacklist:', blcklist)
    blcklist = []
    # Create datasets
    if add_class:
        if torch.cuda.is_available():
            print("Using GPU")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        inputD = torch.empty(0,input_size, sl , device=device)
        targetD = torch.empty(0, zones_num, device=device)
        # maxval = torch.empty(0, input_size, device=device)
        # minval = torch.empty(0, input_size, device=device)
        for i in range(len(df)//17): #len(df)//16
            data = Traj(df[i*17:(i+1)*17], sw, sl, shift)
            for j in range(data.slidlen):
                inpt, trgt = data.slid(j, sw, sl, shift)
                inpt = torch.tensor(inpt, dtype=torch.float32).unsqueeze(0).to(device)
                trgt = trgt.to(device)
                # maxval = torch.cat((maxval, inpt.max(dim=1)[0]), dim=0)
                # minval = torch.cat((minval, inpt.min(dim=1)[0]), dim=0)
                inputD = torch.cat((inputD, inpt), dim=0)
                targetD = torch.cat((targetD, trgt), dim=0)

        # Apply min-max normalization to the data
        print("normalizing data")
        # maxdata = maxval.max(dim=0)[0]
        # mindata = minval.min(dim=0)[0]
        # for r in range(inputD.size(0)):
        #     inputD[r] = (inputD[r] - mindata) / (maxdata - mindata)
        dataset = TrajDataset(inputD, targetD)
        torch.save(dataset, os.path.join(os.getcwd(),'Pickled','datasetTorch.pt'))
        # torch.save([maxdata, mindata], os.path.join(os.getcwd(),'Pickled','normpar.pt'))
        print('dataset saved')
        
    if load_class:
        dataset = torch.load(os.path.join(os.getcwd(),'Pickled','datasetTorch.pt'))
        normpar = torch.load(os.path.join(os.getcwd(),'Pickled','normpar.pt'))
        print('dataset loaded')
    print("Dataset total length is", len(dataset))

    


    if prep_data:

        # Randomly split the data into train, test and validation sets
        train_size = int(0.9 * len(dataset))
        test_size = int(0.1 * len(dataset))
        val_size = len(dataset) - train_size - test_size
        print("Train size:", train_size, "Test size:", test_size, "Validation size:", val_size)

        indices = list(range(len(dataset)))
        random.shuffle(indices)
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]
        val_indices = indices[train_size + test_size:]

        # Define samplers for training and test sets
        train_sampler = SubsetRandomSampler(train_indices)
        test_sampler = SubsetRandomSampler(test_indices)
        val_sampler = SubsetRandomSampler(val_indices)
        # # Create data loaders

        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, sampler=train_sampler)
        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, sampler=test_sampler)
        val_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, sampler=val_sampler)


    if load_model:
        print("Loading model from the checkpoint")
        print('=> Loading checkpoint')
        model = CoordinateRegressionNN(sequence_length, input_size, hidden_size, num_layers, output_size)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        #model, optimizer = load_checkpoint(torch.load("my_checkpoint.pth.tar"), model)
        checkpoint = torch.load("my_checkpoint.pth.tar")
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])

    
    elif model_from_scratch:
        print("Creating model from scratch")
        # Create the model
        model = CoordinateRegressionNN(sequence_length, input_size, hidden_size, num_layers, output_size)
        # Loss and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)


    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs")
        model = nn.DataParallel(model)
    # Move the model to the available GPUs
    model = model.to(device)
        #Training loop


    #Training loop
    if Train:
        predicteddata = torch.empty(0, zones_num, device=device)
        groundtruthy = torch.empty(0, zones_num, device=device)
        groundtruthx = torch.empty(0, input_size, sl, device=device)
        prev_average_loss = 10000000.00000 #float('inf')  # Initialize with a high value
        epoch_losses = []
        test_loss = []
        n = 0
        for epoch in range(epochs):
            model.train()
            train_loss = []
            loss_num = 0
            for x, y in train_loader:
                # if x.size(0) == batch_size:
                loss_num += x.size(0)
                optimizer.zero_grad()
                outputs = model(x)
                # #Normalize the output with the normalization parameters
                # re_mean = normpar[0][:output_size].view(1, 1, output_size)
                # re_var = normpar[1][:output_size].view(1, 1, output_size)
                # y = (y - re_mean) / torch.sqrt(re_var + 1e-5)
                loss = criterion(outputs, y)
                loss.backward()
                optimizer.step()
                train_loss.append(loss.item())
                #epoch_losses.append(loss.item())
            average_loss = sum(train_loss) /loss_num
            epoch_losses.append(average_loss)
            # Saving the best model to the file
            if average_loss < prev_average_loss:
                checkpoint= {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}
                save_checkpoint(checkpoint)
                prev_average_loss = average_loss  # Update previous average loss

            print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss:.5f}')
            if test_in_epoch:
                p,l =test_model(model, test_loader)
                predicteddata = torch.cat((predicteddata, p), dim=0)
                test_loss.append(l)

                if epoch == epochs-1:
                    print(loss)
                    for x, y in test_loader:
                        groundtruthy = torch.cat((groundtruthy, y), dim=0)
                        groundtruthx = torch.cat((groundtruthx, x[:,:input_size,:]), dim=0)
                    savecsvresult(p, groundtruthy,groundtruthx)

        if not test_in_epoch:
            p,l =test_model(model, test_loader)
            predicteddata = torch.cat((predicteddata, p), dim=0)
            test_loss.append(l)

            for x, y in test_loader:
                groundtruthy = torch.cat((groundtruthy, y), dim=0)
                groundtruthx = torch.cat((groundtruthx, x[:,:input_size,:]), dim=0)
        # trainy = torch.empty(0, zones_num, device=device)
        # trainx = torch.empty(0, input_size, sl, device=device)
        # for x, y in train_loader:
        #     trainy = torch.cat((trainy, y), dim=0)
        #     trainx = torch.cat((trainx, x[:,:,:output_size]), dim=0)
        #     savecsvresult(p, groundtruthy,groundtruthx)
        print("Training finished!")
        #save output data as a pickle file
        torch.save(predicteddata, os.path.join(os.getcwd(),'Pickled','ZonePredicteddata.pt'))
        torch.save(test_loss, os.path.join(os.getcwd(),'Pickled','Zonetest_losses.pt'))
        torch.save(epoch_losses, os.path.join(os.getcwd(),'Pickled','Zoneepoch_losses.pt'))
        torch.save([sequence_length, input_size, hidden_size, num_layers, output_size, epoch, batch_size,shift, sw], os.path.join(os.getcwd(),'Pickled','Zonemodel_parameters.pt'))
        torch.save(groundtruthy, os.path.join(os.getcwd(),'Pickled','Zonegroundtruthy.pt'))
        torch.save(groundtruthx, os.path.join(os.getcwd(),'Pickled','Zonegroundtruthx.pt'))
        # torch.save(trainy, os.path.join(os.getcwd(),'Pickled','trainy.pt'))
        # torch.save(trainx, os.path.join(os.getcwd(),'Pickled','trainx.pt'))
        print('data saved')